i get this error when i run the code: Traceback (most recent call last):
  File "C:\Users\aidan_1k98io6\OneDrive - University of Suffolk\projects\AI\data science\Youtube sentiment analysis\main5.py", line 238, in <module>
    main(args.video_ids, args.max_comments)
  File "C:\Users\aidan_1k98io6\OneDrive - University of Suffolk\projects\AI\data science\Youtube sentiment analysis\main5.py", line 169, in main
    df = process_video(video_id, max_comments)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\aidan_1k98io6\OneDrive - University of Suffolk\projects\AI\data science\Youtube sentiment analysis\main5.py", line 107, in process_video
    results = list(tqdm(executor.map(analyze_sentiment, comments), total=len(comments), desc="Analyzing comments"))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\tqdm\std.py", line 1181, in iter
    for obj in iterable:
  File "C:\Python311\Lib\concurrent\futures\_base.py", line 619, in result_iterator
    yield resultor_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\concurrent\futures\_base.py", line 317, in resultor_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\concurrent\futures\_base.py", line 456, in result
    return self.get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\concurrent\futures\_base.py", line 401, in get_result
    raise self._exception
  File "C:\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\aidan_1k98io6\OneDrive - University of Suffolk\projects\AI\data science\Youtube sentiment analysis\main5.py", line 87, in analyze_sentiment
    preprocessed_words = preprocess_text(comment)
                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\aidan_1k98io6\OneDrive - University of Suffolk\projects\AI\data science\Youtube sentiment analysis\main5.py", line 78, in preprocess_text
    return [lemmatizer.lemmatize(word) for word in word_tokens if word.isalnum() and word not in stop_words]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\aidan_1k98io6\OneDrive - University of Suffolk\projects\AI\data science\Youtube sentiment analysis\main5.py", line 78, in <listcomp>
    return [lemmatizer.lemmatize(word) for word in word_tokens if word.isalnum() and word not in stop_words]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\nltk\stem\wordnet.py", line 45, in lemmatize
    lemmas = wn._morphy(word, pos)
             ^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\nltk\corpus\util.py", line 121, in getattr**
    self.load()
  File "C:\Python311\Lib\site-packages\nltk\corpus\util.py", line 95, in load
    args, kwargs = self.args, self.kwargs
                   ^^^^^^^^^^^
AttributeError: 'WordNetCorpusReader' object has no attribute '_LazyCorpusLoader__args' EditI apologize for the error. It seems there might be an issue with the NLTK WordNet corpus. Let's try to fix this by explicitly downloading the WordNet corpus and modifying our code to handle potential NLTK-related errors more gracefully. Here's an updated version of the script that should resolve this issue:
Fixed YouTube Comment Sentiment Analysis ScriptClick to open code
Key changes made to address the error:

Added explicit download for 'omw-1.4' NLTK data, which is required for WordNet to function properly in newer versions of NLTK.
Added error handling around NLTK data downloads.
Modified the preprocess_text function to handle potential NLTK-related errors and fall back to basic preprocessing if necessary.
Added error handling in the analyze_sentiment function to prevent the entire process from crashing if there's an issue with a single comment.
Modified the process_video function to filter out None results from analyze_sentiment.

These changes should make the script more robust and able to handle potential NLTK-related issues. If you encounter any further errors, please let me know, and I'll be happy to help you resolve them.
To run the script, use the same command as before:
Copypython script_name.py video_id1 video_id2 video_id3 --max_comments 1000
If you still encounter issues, it might be helpful to reinstall NLTK:
Copypip uninstall nltk
pip install nltk
Then run the script again. 